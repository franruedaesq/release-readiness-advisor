import { Injectable, Logger } from '@nestjs/common';
import { ConfigService } from '@nestjs/config';
import { StateGraph, END, START } from '@langchain/langgraph';
import { Annotation } from '@langchain/langgraph';
import { BaseMessage } from '@langchain/core/messages';
import { ChatOpenAI } from '@langchain/openai';
import { DynamicTool } from '@langchain/core/tools';
import { ChromaClient } from 'chromadb';
import { DefaultEmbeddingFunction } from '@chroma-core/default-embed';
import { MetricsService } from 'src/metrics/metrics.service';

// 1. Define the State that will flow through the graph
const AgentState = Annotation.Root({
  // The initial question or instruction
  input: Annotation<string>(),
  // The context retrieved from our ChromaDB database
  context: Annotation<string>({
    reducer: (_prev, next) => next,
    default: () => '',
  }),
  // The risk analysis and score generated by the RiskAgent
  riskAnalysis: Annotation<string>({
    reducer: (_prev, next) => next,
    default: () => '',
  }),
  // The deployment and rollback plan generated by the OpsAgent
  deploymentPlan: Annotation<string>({
    reducer: (_prev, next) => next,
    default: () => '',
  }),
  // The final report in Markdown
  report: Annotation<string>({
    reducer: (_prev, next) => next,
    default: () => '',
  }),
  // The workflow run ID we are analyzing
  runId: Annotation<number>(),
});

@Injectable()
export class AgentOrchestratorService {
  private readonly logger = new Logger(AgentOrchestratorService.name);
  private graph: StateGraph<typeof AgentState.State>;
  private compiledGraph: any; // The compiled and executable graph
  private chroma: ChromaClient;
  private embeddingFunction = new DefaultEmbeddingFunction();

  constructor(
    private readonly configService: ConfigService,
    private readonly metricsService: MetricsService,
  ) {
    const chromaUrl = this.configService.get<string>(
      'CHROMA_URL',
      'http://localhost:8000',
    );
    this.chroma = new ChromaClient({ path: chromaUrl });

    this.buildGraph();
  }

  private buildGraph() {
    // Create the graph and chain all definitions
    const workflow = new StateGraph(AgentState)
      // 1. Define the Nodes of our graph (our agents)
      .addNode('retrieve_context', this.retrieveContextNode.bind(this))
      .addNode('risk_agent', this.runRiskAgentNode.bind(this))
      .addNode('ops_agent', this.runOpsAgentNode.bind(this))
      .addNode('writer_agent', this.runWriterAgentNode.bind(this))

      // 2. Define the transitions (the workflow)
      .addEdge(START, 'retrieve_context')
      .addEdge('retrieve_context', 'risk_agent')
      .addEdge('risk_agent', 'ops_agent')
      .addEdge('ops_agent', 'writer_agent')
      .addEdge('writer_agent', END);

    // 3. Compile the graph
    this.compiledGraph = workflow.compile();
    this.logger.log('Agent graph compiled successfully.');
  }

  // --- Graph Nodes ---

  private async retrieveContextNode(
    state: typeof AgentState.State,
  ): Promise<Partial<typeof AgentState.State>> {
    this.logger.log(`Node: retrieve_context for runId ${state.runId}`);
    const collection = await this.chroma.getOrCreateCollection({
      name: 'release_artifacts',
      embeddingFunction: this.embeddingFunction,
    });

    const results = await collection.query({
      queryTexts: [state.input],
      nResults: 5,
      where: { runId: state.runId },
    });

    const context = results.documents[0].join('\n\n---\n\n');
    return { context };
  }

  private async runRiskAgentNode(
    state: typeof AgentState.State,
    config: { configurable: { model: string } },
  ): Promise<Partial<typeof AgentState.State>> {
    this.logger.log('Node: risk_agent');
    const model = new ChatOpenAI({
      model: config.configurable.model,
    });
    const prompt = `Based on the following test results and security scans, assess the risk of the release. Provide a risk score from 0 to 100 and a clear, concise justification.
    
  Context:
  ${state.context}`;

    const response = await model.invoke(prompt);
    console.log({ response });
    const usage = response.usage_metadata;
    if (usage) {
      this.metricsService.recordLlmUsage(
        {
          promptTokens: usage.input_tokens,
          completionTokens: usage.output_tokens,
        },
        'risk_agent',
        config.configurable.model,
      );
    }
    return { riskAnalysis: response.content as string };
  }

  private async runOpsAgentNode(
    state: typeof AgentState.State,
    config: { configurable: { model: string } },
  ): Promise<Partial<typeof AgentState.State>> {
    this.logger.log('Node: ops_agent');
    const model = new ChatOpenAI({
      model: config.configurable.model,
    });
    const prompt = `A risk analysis has determined the following:
  ${state.riskAnalysis}

  Based on this analysis, propose a deployment plan (e.g. 'Full', 'Canary 25%', 'Blue-Green') and an emergency rollback plan. Be brief and direct.`;

    const response = await model.invoke(prompt);
    console.log({ response });
    const usage = response.usage_metadata;
    if (usage) {
      this.metricsService.recordLlmUsage(
        {
          promptTokens: usage.input_tokens,
          completionTokens: usage.output_tokens,
        },
        'ops_agent',
        config.configurable.model,
      );
    }
    return { deploymentPlan: response.content as string };
  }

  private async runWriterAgentNode(
    state: typeof AgentState.State,
    config: { configurable: { model: string } },
  ): Promise<Partial<typeof AgentState.State>> {
    this.logger.log('Node: writer_agent');
    const model = new ChatOpenAI({
      model: config.configurable.model,
    });

    const prompt = `Summarize all the following information in a release readiness report in Markdown format. Be professional and clear.
    
  Risk Analysis:
  ${state.riskAnalysis}

  Operations Plan:
  ${state.deploymentPlan}

  Raw Evidence:
  ${state.context}`;

    const response = await model.invoke(prompt);
    console.log({ response });
    const usage = response.usage_metadata;
    if (usage) {
      this.metricsService.recordLlmUsage(
        {
          promptTokens: usage.input_tokens,
          completionTokens: usage.output_tokens,
        },
        'writer_agent',
        config.configurable.model,
      );
    }
    return { report: response.content as string };
  }

  // --- Main Invocation Method ---

  public async invoke(
    runId: number,
    task: string,
    model: string,
  ): Promise<string> {
    // The initial 'input' now comes from the UI task selection
    const initialState: typeof AgentState.State = {
      runId: runId,
      input: task, // Use the task from the UI
      context: '',
      riskAnalysis: '',
      deploymentPlan: '',
      report: '',
    };

    const finalState = await this.compiledGraph.invoke(
      initialState,
      { configurable: { model: model } }, // Pass model in config
    );
    await this.metricsService.judgeAnswerAlignment(
      finalState.input,
      finalState.context,
      finalState.report,
      model,
    );

    return finalState.report;
  }
}
